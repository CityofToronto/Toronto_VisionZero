{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# This dataframe is the output of the notebook 'TorontoCollisions_EDA', which \n# shows the collisions from 2000 to 2019 in INSIGHT ('Church-Yonge Corridor (75)') neighborhood.\ngeo_dfE_I = pd.read_csv(\"../input/geo_dfE_I.csv\")\ngeo_dfE_I.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"geo_dfE_I[['# involved', '# injured', '# fatalities', '# KSI']].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Percentage of '# involved' in each collision\ninv_per = geo_dfE_I['# involved'].value_counts(normalize=True) * 100\ninv_per = pd.DataFrame({'percentage':inv_per})\ninv_per","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Bar(\n    x=inv_per.index, y=inv_per['percentage'],\n    text=inv_per['percentage'],\n    textposition='outside',\n    texttemplate='%{text:.2f}'+'%')])\nfig.update_layout(title='',\n                  xaxis=dict(\n                      title='Number of people involved in each collision',\n                      tickmode = 'linear',\n                      tick0 = 1,\n                      dtick = 1), \n                  yaxis=dict(\n                      title='Percentage',\n                      tickmode = 'array',\n                      tickvals = [0]))\nfig.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creata a new dataframe for doing regression prediction\ndf_reg = geo_dfE_I[['acc_date', 'road_surface_cond', 'visibility', 'light', '# involved']]\n\ndf_reg['acc_date'] = pd.to_datetime(df_reg['acc_date'], errors = 'raise')\ndf_reg['year']=df_reg['acc_date'].dt.year\ndf_reg['month']=df_reg['acc_date'].dt.month\n#import calendar\n# df_reg['month_name'] = df_reg['month'].apply(lambda x: calendar.month_abbr[x])\ndf_reg['month_name'] = df_reg['acc_date'].dt.month_name()\ndf_reg['day_of_week'] = df_reg['acc_date'].dt.day_name()\n\ndf_reg.drop('acc_date', axis=1, inplace=True)\ndf_reg","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# one-hot encoding\n**We need to one-hot encode our categorical features: 'road_surface_cond', 'visibility', 'light', and 'month'.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_reg['road_surface_cond'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_reg['visibility'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(df_reg['light'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ohe stands for one-hot encoding\nohe_fields=['road_surface_cond','visibility','light','month', 'day_of_week']\n\n# One-Hot encode a couple of variables\ndf_reg = pd.get_dummies(df_reg, columns=ohe_fields, prefix=ohe_fields)\ndf_reg","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Set the train and test sets\ndf_reg_2018 = df_reg[df_reg['year']==2018]  # test\ndf_reg_other = df_reg[df_reg['year']!=(2018 & 2019)]  # train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_reg.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train = df_reg_other[['road_surface_cond_DRY',\n       'road_surface_cond_ICE', 'road_surface_cond_LOOSE SAND OR GRAVEL',\n       'road_surface_cond_LOOSE SNOW', 'road_surface_cond_OTHER',\n       'road_surface_cond_PACKED SNOW', 'road_surface_cond_SLUSH',\n       'road_surface_cond_SPILLED LIQUID', 'road_surface_cond_WET',\n       'visibility_CLEAR', 'visibility_DRIFTING SNOW',\n       'visibility_FOG, MIST, SMOKE, DUST', 'visibility_FREEZING RAIN',\n       'visibility_OTHER', 'visibility_RAIN', 'visibility_SNOW',\n       'visibility_STRONG WIND', 'light_DARK', 'light_DARK, ARTIFICIAL',\n       'light_DAWN', 'light_DAWN, ARTIFICIAL', 'light_DAYLIGHT',\n       'light_DAYLIGHT, ARTIFICIAL', 'light_DUSK', 'light_DUSK, ARTIFICIAL',\n       'light_OTHER', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',\n       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n       'month_12', 'day_of_week_Friday', 'day_of_week_Monday',\n       'day_of_week_Saturday', 'day_of_week_Sunday', 'day_of_week_Thursday',\n       'day_of_week_Tuesday', 'day_of_week_Wednesday']].values\n\nY_train = df_reg_other['# involved'].values\n\nX_test = df_reg_2018[['road_surface_cond_DRY',\n       'road_surface_cond_ICE', 'road_surface_cond_LOOSE SAND OR GRAVEL',\n       'road_surface_cond_LOOSE SNOW', 'road_surface_cond_OTHER',\n       'road_surface_cond_PACKED SNOW', 'road_surface_cond_SLUSH',\n       'road_surface_cond_SPILLED LIQUID', 'road_surface_cond_WET',\n       'visibility_CLEAR', 'visibility_DRIFTING SNOW',\n       'visibility_FOG, MIST, SMOKE, DUST', 'visibility_FREEZING RAIN',\n       'visibility_OTHER', 'visibility_RAIN', 'visibility_SNOW',\n       'visibility_STRONG WIND', 'light_DARK', 'light_DARK, ARTIFICIAL',\n       'light_DAWN', 'light_DAWN, ARTIFICIAL', 'light_DAYLIGHT',\n       'light_DAYLIGHT, ARTIFICIAL', 'light_DUSK', 'light_DUSK, ARTIFICIAL',\n       'light_OTHER', 'month_1', 'month_2', 'month_3', 'month_4', 'month_5',\n       'month_6', 'month_7', 'month_8', 'month_9', 'month_10', 'month_11',\n       'month_12', 'day_of_week_Friday', 'day_of_week_Monday',\n       'day_of_week_Saturday', 'day_of_week_Sunday', 'day_of_week_Thursday',\n       'day_of_week_Tuesday', 'day_of_week_Wednesday']].values\n\nY_test = df_reg_2018['# involved'].values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Normalize Data\n# Data Standardization give data zero mean and unit variance, it is good practice, especially for algorithms such as KNN which is based on distance of cases:\nfrom sklearn import preprocessing\nX_train = preprocessing.StandardScaler().fit(X_train).transform(X_train.astype(float))\nX_test = preprocessing.StandardScaler().fit(X_test).transform(X_test.astype(float))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Test set:', X_test.shape,  Y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Multiple Linear Regression\n# from sklearn import linear_model\n# regr = linear_model.LinearRegression()\n# regr.fit(X_train, Y_train)\n# # The coefficients\n# print ('Coefficients: ', regr.coef_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Y_hat= regr.predict(X_test)\n# print(\"Mean squared error (MSE): %.2f\"\n#       % np.mean((Y_hat - Y_test) ** 2))\n\n# # Explained variance score: 1 is perfect prediction\n# print('Variance score: %.2f' % regr.score(X_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Polynomial regression\n\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.metrics import r2_score\n\n# poly = PolynomialFeatures(degree=5)\n# X_train_poly = poly.fit_transform(X_train)\n\n# clf = linear_model.LinearRegression()\n# Y_hat_train = clf.fit(X_train_poly, Y_train)\n\n# The coefficients\n# print ('Coefficients: ', clf.coef_)\n# print ('Intercept: ', clf.intercept_)\n\n# X_test_poly = poly.fit_transform(X_test)\n# Y_hat_test = clf.predict(X_test_poly)\n# print(\"Mean absolute error: %.2f\" % np.mean(np.absolute(Y_hat_test - Y_test)))\n# print(\"Mean squared error (MSE): %.2f\" % np.mean((Y_hat_test - Y_test) ** 2))\n# print(\"R2-score: %.2f\" % r2_score(Y_hat_test, Y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluation**\n\nwe compare the actual values and predicted values to calculate the accuracy of a regression model. Evaluation metrics provide a key role in the development of a model, as it provides insight to areas that require improvement.\nThere are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set:\n- **Mean absolute error**: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it’s just average error.\n- **Mean Squared Error (MSE)**: It is the mean of the squared error. It’s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.\n- **Root Mean Squared Error (RMSE)**: This is the square root of the Mean Square Error.\n- **R-squared** is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Random Forest Regression\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Instantiate model with 1000 decision trees\nrfr = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n\n# Train the model on training data\nrfr.fit(X_train, Y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the random forest's predict method on the test data\nY_hat_test = rfr.predict(X_test)\n\n# Calculate the absolute errors\nerrors = abs(Y_hat_test - Y_test)\n\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors / Y_test)\n\n# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Y_hat_test.mean()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}